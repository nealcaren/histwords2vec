{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "vertical-river",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gensim.models import KeyedVectors\n",
    "import pickle\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "approved-environment",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shrink(root):\n",
    "    '''\n",
    "    Translate HistWords keyed vector files to word2vec_format\n",
    "    \n",
    "    Based on gist by @chengjun\n",
    "    https://github.com/williamleif/histwords/issues/8#issuecomment-400155072\n",
    "    \n",
    "    Note: removes vocabulary without vectors (all 0s in original)\n",
    "    '''\n",
    "    \n",
    "    # Load the original files\n",
    "    mat = np.load(f\"{root}-w.npy\", \n",
    "                  mmap_mode=\"c\")\n",
    "\n",
    "    iw = pickle.load(open(f'{root}-vocab.pkl','rb'))\n",
    "    \n",
    "    # convert to a pandas dataframe\n",
    "    df = pd.DataFrame(mat, index=iw)\n",
    "    \n",
    "    # figure out which rows are all 0s and drop them\n",
    "    df['sum'] = df.sum(axis=1)\n",
    "    sdf = df[df['sum']!=0].copy()\n",
    "    \n",
    "    vocab_size  = len(sdf)\n",
    "    vector_size = 300\n",
    "    \n",
    "    print(f'{vocab_size} words with {vector_size} vectors.')\n",
    "    with open(f\"{root}-sm.txt\", \"w\") as fp:\n",
    "        # write header\n",
    "        fp.write(str(vocab_size) + \" \" + str(vector_size) + \"\\n\")\n",
    "        # write vectors\n",
    "        for row in sdf.iterrows():\n",
    "            word =row[0]\n",
    "            vectors = [str(i) for i in row[1][:300]]\n",
    "            fp.write(word + \" \" + \" \".join(vectors) + \"\\n\")\n",
    "            \n",
    "    # load in gensim\n",
    "    m = KeyedVectors.load_word2vec_format(f\"{root}-sm.txt\",\n",
    "                                          binary=False)  \n",
    "    # save in word2vec C format\n",
    "    m.save_word2vec_format(f'{root}-sm.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "restricted-evanescence",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1800\n",
      "13045 words with 300 vectors.\n",
      "1810\n",
      "15771 words with 300 vectors.\n",
      "1820\n",
      "20312 words with 300 vectors.\n",
      "1830\n",
      "21691 words with 300 vectors.\n",
      "1840\n",
      "23818 words with 300 vectors.\n",
      "1850\n",
      "29035 words with 300 vectors.\n",
      "1860\n",
      "27191 words with 300 vectors.\n",
      "1870\n",
      "29320 words with 300 vectors.\n",
      "1880\n",
      "34081 words with 300 vectors.\n",
      "1890\n",
      "37729 words with 300 vectors.\n",
      "1900\n",
      "41551 words with 300 vectors.\n",
      "1910\n",
      "36553 words with 300 vectors.\n",
      "1920\n",
      "35643 words with 300 vectors.\n",
      "1930\n",
      "34477 words with 300 vectors.\n",
      "1940\n",
      "34226 words with 300 vectors.\n",
      "1950\n",
      "41807 words with 300 vectors.\n",
      "1960\n",
      "54332 words with 300 vectors.\n",
      "1970\n",
      "60344 words with 300 vectors.\n",
      "1980\n",
      "64934 words with 300 vectors.\n",
      "1990\n",
      "71097 words with 300 vectors.\n"
     ]
    }
   ],
   "source": [
    "#process all files\n",
    "\n",
    "for year in range(1800,2000,10):\n",
    "    print(year)\n",
    "    root = f'eng-all_sgns/{year}'\n",
    "    shrink(root)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
